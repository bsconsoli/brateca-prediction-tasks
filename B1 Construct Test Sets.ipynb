{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d91173",
   "metadata": {},
   "source": [
    "## Modules + File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "biobert = 'D:/Downloads/LengthOfStay_Tests/pucpr/biobertpt-all' #folder where the BioBertPT files were unzipped\n",
    "folder_path = 'D:/Downloads/Results/Stage_6' #folder where the BRATECA V1 files were unzipped\n",
    "\n",
    "with (open(folder_path+'/exam_feature_dict.pkl', \"rb\")) as openfile:\n",
    "    adm_exam_features = pickle.load(openfile)    \n",
    "with (open(folder_path+'/note_dict.pkl', \"rb\")) as openfile:\n",
    "    cn_features = pickle.load(openfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0633c1",
   "metadata": {},
   "source": [
    "## Constructing Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d77888",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_keys = set(adm_exam_features.keys()) & set(cn_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1228f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(exam_dict, note_dict, patient_keys, tokenizer):\n",
    "    exam_features, note_tokens, token_masks, los_labels, mort_labels = [],[],[],[],[]\n",
    "    for key in patient_keys:\n",
    "        #exam input\n",
    "        curr_exam_features = exam_dict[key][0]\n",
    "        \n",
    "        #bert input\n",
    "        concatenated_note = ''\n",
    "        for note in note_dict[key][0]:\n",
    "            concatenated_note = concatenated_note + note[0]\n",
    "        preprocessed_note = re.sub('[@#$%&*<>]',' ',concatenated_note)\n",
    "        inputs = tokenizer.encode_plus(preprocessed_note, add_special_tokens=True, max_length=512, padding='max_length', \n",
    "                                             return_attention_mask=True, truncation=True)\n",
    "        #append to NN input arrays\n",
    "        exam_features.append(curr_exam_features)\n",
    "        note_tokens.append(inputs['input_ids'])\n",
    "        token_masks.append(inputs['attention_mask'])\n",
    "        if exam_dict[key][1]:\n",
    "            los_labels.append(1)\n",
    "        else:\n",
    "            los_labels.append(0)\n",
    "        if exam_dict[key][2] == 'Alta':\n",
    "            mort_labels.append(0)\n",
    "        elif exam_dict[key][2] == 'Obito':\n",
    "            mort_labels.append(1)\n",
    "        else:\n",
    "            mort_labels.append(2)\n",
    "        \n",
    "    return np.asarray(exam_features, dtype='float64'), np.asarray(note_tokens, dtype='int32'), np.asarray(token_masks, dtype='int32'), np.asarray(los_labels, dtype='int32'), np.asarray(mort_labels, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(biobert, do_lower_case='True', truncation_side='left',\n",
    "                                         padding_side='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_features, note_tokens, token_masks, los_labels, mort_labels = tokenize(adm_exam_features,cn_features,\n",
    "                                                                                shared_keys,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_exam_features = None\n",
    "cn_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b9498",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of Stay Prediction\\nLabel 0 = Less or equal to 7 days\\nLabel 1 = More than 7 days\\n')\n",
    "print('Quantity of Label 0 Examples:', len([x for x in los_labels if x == 0]))\n",
    "print('Quantity of Label 1 Examples:', len([x for x in los_labels if x == 1]))\n",
    "print('Total Examples:', len(los_labels))\n",
    "los_prop = round(len([x for x in los_labels if x == 1])/len([x for x in los_labels if x == 0]),2)\n",
    "print('Positive-to-Negative: ',los_prop,':1',sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mortality Prediction\\nLabel 0 = Discharge\\nLabel 1 = Death\\n')\n",
    "print('Quantity of Label 0 Examples:', len([x for x in mort_labels if x == 0]))\n",
    "print('Quantity of Label 1 Examples:', len([x for x in mort_labels if x == 1]))\n",
    "print('Total Examples:', len(mort_labels))\n",
    "mort_prop = round(len([x for x in mort_labels if x == 1])/len([x for x in mort_labels if x == 0]),2)\n",
    "print('Positive-to-Negative: ',mort_prop,':1',sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "#balance dataset\n",
    "def balance_dataset(labels):\n",
    "    count = 0 #counter for number of negative-pairs in balanced dataset that doesn't have a positive-pair\n",
    "    indexes_false = [] #indexes for negative pairs in balanced dataset\n",
    "    indexes_true = []  #indexes for positive pairs in balanced dataset\n",
    "    indexes_rest = []  #indexes for pairs not in balanced dataset\n",
    "    for i,label in enumerate(labels): #iterate over labels, retrieve label and index of label\n",
    "        if label == 1: #since there are less positives, always add them to balanced dataset\n",
    "            indexes_true.append(i)\n",
    "            count +=1\n",
    "        elif label == 0 and count > 0: #if dataset in unbalanced, add negative to balanced dataset\n",
    "            indexes_false.append(i)\n",
    "            count -=1\n",
    "        else: #if dataset is balanced, do not add negative pair to dataset\n",
    "            indexes_rest.append(i)\n",
    "    while len(indexes_true) > len(indexes_false): #if dataset in unbalanced by the end of the loop, add more negatives\n",
    "        indexes_false.append(indexes_rest.pop(-1))\n",
    "    return indexes_false, indexes_true, indexes_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "los_false, los_true, los_rest = balance_dataset(los_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4dbab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of Stay Prediction\\nLabel 0 = Less or equal to 7 days\\nLabel 1 = More than 7 days\\n')\n",
    "print('Balanced Quantity of Label 0:',len(los_false))\n",
    "print('Balanced Quantity of Label 1:',len(los_true))\n",
    "print('Total Balanced Examples:', len(los_false)+len(los_true))\n",
    "print('Total Left-over Examples:', len(los_rest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23d9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "mort_false, mort_true, mort_rest = balance_dataset(mort_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mortality Prediction\\nLabel 0 = Discharge\\nLabel 1 = Death\\n')\n",
    "print('Balanced Quantity of Label 0:',len(mort_false))\n",
    "print('Balanced Quantity of Label 1:',len(mort_true))\n",
    "print('Total Balanced Examples:', len(mort_false)+len(mort_true))\n",
    "print('Total Left-over Examples:', len(mort_rest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join features and labels into the test sets by using the indexes\n",
    "def make_datasets(indexes_true, indexes_false, indexes_rest,exam_features,note_tokens,token_masks,labels,prop):\n",
    "    TRAIN_PERCENTAGE = 0.7 #percentage of dataset that will be used to train the model\n",
    "    VAL_PERCENTAGE = 0.1 #percentage of dataset that will be used to validate the model\n",
    "    train_proportion = round(len(indexes_false)*TRAIN_PERCENTAGE) #number of pairs in train dataset, based on train_percentage\n",
    "    val_proportion = train_proportion+round(len(indexes_false)*VAL_PERCENTAGE)\n",
    "\n",
    "    #balanced train and test datasets\n",
    "    balanced_train_features = []\n",
    "    balanced_train_masks = []\n",
    "    balanced_train_exam = []\n",
    "    balanced_train_labels = []\n",
    "    balanced_test_features = []\n",
    "    balanced_test_masks = []\n",
    "    balanced_test_exam = []\n",
    "    balanced_test_labels = []\n",
    "    balanced_val_features = []\n",
    "    balanced_val_masks = []\n",
    "    balanced_val_exam = []\n",
    "    balanced_val_labels = []\n",
    "\n",
    "    for i,key in enumerate(indexes_false): #separate balanced pairs into train and test sets\n",
    "        if i < train_proportion:\n",
    "            balanced_train_features.append(note_tokens[key])\n",
    "            balanced_train_masks.append(token_masks[key])\n",
    "            balanced_train_exam.append(exam_features[key])\n",
    "            balanced_train_labels.append(labels[key])\n",
    "            balanced_train_features.append(note_tokens[indexes_true[i]])\n",
    "            balanced_train_masks.append(token_masks[indexes_true[i]])\n",
    "            balanced_train_exam.append(exam_features[indexes_true[i]])\n",
    "            balanced_train_labels.append(labels[indexes_true[i]])\n",
    "        elif i < val_proportion:\n",
    "            balanced_val_features.append(note_tokens[key])\n",
    "            balanced_val_masks.append(token_masks[key])\n",
    "            balanced_val_exam.append(exam_features[key])\n",
    "            balanced_val_labels.append(labels[key])\n",
    "            balanced_val_features.append(note_tokens[indexes_true[i]])\n",
    "            balanced_val_masks.append(token_masks[indexes_true[i]])\n",
    "            balanced_val_exam.append(exam_features[indexes_true[i]])\n",
    "            balanced_val_labels.append(labels[indexes_true[i]])\n",
    "        else:\n",
    "            balanced_test_features.append(note_tokens[key])\n",
    "            balanced_test_masks.append(token_masks[key])\n",
    "            balanced_test_exam.append(exam_features[key])\n",
    "            balanced_test_labels.append(labels[key])\n",
    "            balanced_test_features.append(note_tokens[indexes_true[i]])\n",
    "            balanced_test_masks.append(token_masks[indexes_true[i]])\n",
    "            balanced_test_exam.append(exam_features[indexes_true[i]])\n",
    "            balanced_test_labels.append(labels[indexes_true[i]])\n",
    "\n",
    "    #proportional test dataset, copied from the balanced set\n",
    "    proportional_test_features = balanced_test_features.copy()\n",
    "    proportional_test_masks = balanced_test_masks.copy()\n",
    "    proportional_test_exam = balanced_test_exam.copy()\n",
    "    proportional_test_labels = balanced_test_labels.copy()\n",
    "\n",
    "    negative_pairs_in_testset = len(balanced_test_labels)/2\n",
    "\n",
    "    for key in indexes_rest: #add positive pairs to copy of original balanced test set to make proportional test set\n",
    "        if len(proportional_test_labels)-negative_pairs_in_testset < round((len(balanced_test_labels)/2)/prop):\n",
    "            proportional_test_features.append(note_tokens[key])\n",
    "            proportional_test_masks.append(token_masks[key])\n",
    "            proportional_test_exam.append(exam_features[key])\n",
    "            proportional_test_labels.append(labels[key])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    #turn all lists into numpy arrays for use with TensorFlow\n",
    "    balanced_train_features = np.array(balanced_train_features)\n",
    "    balanced_train_masks = np.array(balanced_train_masks)\n",
    "    balanced_train_exam = np.array(balanced_train_exam)\n",
    "    balanced_train_labels = np.array(balanced_train_labels)\n",
    "    training_data = [balanced_train_features,balanced_train_masks,balanced_train_exam,balanced_train_labels]\n",
    "    \n",
    "    balanced_test_features = np.array(balanced_test_features)\n",
    "    balanced_test_masks = np.array(balanced_test_masks)\n",
    "    balanced_test_exam = np.array(balanced_test_exam)\n",
    "    balanced_test_labels = np.array(balanced_test_labels)\n",
    "    testing_data = [balanced_test_features,balanced_test_masks,balanced_test_exam,balanced_test_labels]\n",
    "    \n",
    "    balanced_val_features = np.array(balanced_val_features)\n",
    "    balanced_val_masks = np.array(balanced_val_masks)\n",
    "    balanced_val_exam = np.array(balanced_val_exam)\n",
    "    balanced_val_labels = np.array(balanced_val_labels)\n",
    "    val_data = [balanced_val_features,balanced_val_masks,balanced_val_exam,balanced_val_labels]\n",
    "    \n",
    "    proportional_test_features = np.array(proportional_test_features)\n",
    "    proportional_test_masks = np.array(proportional_test_masks)\n",
    "    proportional_test_exam = np.array(proportional_test_exam)\n",
    "    proportional_test_labels = np.array(proportional_test_labels)\n",
    "    prop_testing_data = [proportional_test_features,proportional_test_masks,proportional_test_exam,proportional_test_labels]\n",
    "    \n",
    "    return [training_data, testing_data, val_data, prop_testing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a062c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "los_testset = make_datasets(los_true, los_false, los_rest, exam_features, note_tokens, token_masks, los_labels,los_prop)\n",
    "mort_testset = make_datasets(mort_true, mort_false, mort_rest, exam_features, note_tokens, token_masks, mort_labels,mort_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f37d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(balanced_train_labels,balanced_test_labels,proportional_test_labels):\n",
    "    print('Total Balanced Train Examples:',len(balanced_train_labels))\n",
    "    balanced_negative_pairs = len([x for x in balanced_train_labels if x == 0])\n",
    "    balanced_positive_pairs = len([x for x in balanced_train_labels if x == 1])\n",
    "    print('Balanced Training Positive-to-Negative: ',round(balanced_positive_pairs/balanced_negative_pairs,2),':1.0',sep='')\n",
    "    print('Total Balanced Test Examples:',len(balanced_test_labels))\n",
    "    balanced_negative_pairs = len([x for x in balanced_test_labels if x == 0])\n",
    "    balanced_positive_pairs = len([x for x in balanced_test_labels if x == 1])\n",
    "    print('Balanced Testing Positive-to-Negative: ',round(balanced_positive_pairs/balanced_negative_pairs,2),':1.0',sep='')\n",
    "    print('Total Proportional Test Examples:',len(proportional_test_labels))\n",
    "    proportional_negative_pairs = len([x for x in proportional_test_labels if x == 0])\n",
    "    proportional_positive_pairs = len([x for x in proportional_test_labels if x == 1])\n",
    "    print('Proportional Positive-to-Negative: ',round(proportional_positive_pairs/proportional_negative_pairs,2),':1.0',sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of Stay Prediction\\nLabel 0 = Less or equal to 7 days\\nLabel 1 = More than 7 days\\n')\n",
    "print_stats(los_testset[0][3],los_testset[1][3],los_testset[3][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df648721",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mortality Prediction\\nLabel 0 = Discharge\\nLabel 1 = Death\\n')\n",
    "print_stats(mort_testset[0][3],mort_testset[1][3],mort_testset[3][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55466674",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path+'/los_testset.pkl', 'wb') as note_file:\n",
    "    pickle.dump(los_testset, note_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(folder_path+'/mort_testset.pkl', 'wb') as note_file:\n",
    "    pickle.dump(mort_testset, note_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
